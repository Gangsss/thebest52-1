{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, datapath):\n",
    "        ''' \n",
    "        Load data from the DB Books\n",
    "        List the users and items\n",
    "        List all the users historic\n",
    "        '''\n",
    "        self.data = self.load_datas(datapath)\n",
    "        self.users = self.data['user'].unique()\n",
    "        self.items = self.data['item'].unique()\n",
    "        self.histo = self.gen_histo()\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "        \n",
    "\n",
    "    def load_datas(self, datapath):\n",
    "        '''\n",
    "        Load the data and merge the name of each books\n",
    "        A row corresponds to a rate given by a user to books\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapath:   string, path to the data books contain user, item, rating, timestamp\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result:     DataFram, contains all the ratings\n",
    "        '''\n",
    "        data = pd.read_csv(datapath, names=['item', 'user', 'rating', 'timestamp'])\n",
    "        data = data[:1000]\n",
    "        print(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def gen_histo(self):\n",
    "        '''\n",
    "        Group all rates given by users and store them from older to most recent\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result:     List(DataFrame), List of the historic for each user\n",
    "        '''\n",
    "        historic_user = []\n",
    "        for i, u in enumerate(self.users):\n",
    "            temp = self.data[self.data['user'] == u]\n",
    "            temp = temp.sort_values('timestamp').reset_index()\n",
    "            temp.drop('index', axis=1, inplace=True)\n",
    "            historic_user.append(temp)\n",
    "        return historic_user\n",
    "    \n",
    "    def sample_histo(self, user_histo, action_ratio=0.8, \n",
    "                     max_samp_by_user=5, max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
    "        '''\n",
    "        action이 이 코드에 따르면 각 아이템에 대한 rating이 action인 것으로 보이는데 book이어야 하는 것아닌가?\n",
    "        '''\n",
    "        '''\n",
    "        For a given historic, make one or multiple sampling.\n",
    "        If no optional argument given for nb_states and nb_actions, \n",
    "        then the sampling is random and each sample can have differents size for action and state.\n",
    "        To normalize sampling we need to give list of the numbers of states and actions to be sampled\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_histo:         DataFrame, historic of user\n",
    "        delimiter:          float, optional delimiter for the csv\n",
    "        action_ratio:       float, optional ratio form which books in history will be selected\n",
    "        max_samp_by_user:   int, optional Number max of sample to make by user\n",
    "        max_state:          int, optional Number max of books to take for the 'state' column\n",
    "        max_action:         int, optional Number max of books to take for the 'action' column\n",
    "        nb_state:           array(int), optional Numbers of books to be taken for each sample made on user's historic\n",
    "        nb_actions:         array(int), optional Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states:             List(String), All the states sampled, format of a sample: item & rating\n",
    "        actions:            List(String), All the actions sampled, format of a sample: item & rating\n",
    "\n",
    "        Notes\n",
    "        -------\n",
    "        States must be before(timestamp) the actions.\n",
    "        If given, size of nb_states is the number of sample by user size of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "\n",
    "        n = len(user_histo)\n",
    "        print(n)\n",
    "        sep = int(action_ratio * n)\n",
    "        nb_sample = random.randint(1, max_samp_by_user)\n",
    "        if not nb_states:\n",
    "            nb_states = [min(random.randint(1, sep), max_state) for i in range(nb_sample)]\n",
    "        if not nb_actions:\n",
    "            nb_actions = [min(random.randint(1, n-sep), max_action) for i in range(nb_sample)]\n",
    "        \n",
    "        assert len(nb_states) == len(nb_actions)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "        # SELECT SAMPLES IN HISTO\n",
    "        for i in range(len(nb_states)):\n",
    "            sample_states = user_histo.iloc[0:sep].sample(nb_states[i])\n",
    "            sample_actions = user_histo.iloc[-(n-sep):].sample(nb_actions[i])\n",
    "\n",
    "            sample_state = []\n",
    "            sample_action = []\n",
    "            for j in range(nb_states[i]):\n",
    "                row = sample_states.iloc[j]\n",
    "                # FORMAT STATE\n",
    "                state = str(row.loc['item']) + '&' + str(row.loc['rating'])\n",
    "                sample_state.append(state)\n",
    "            \n",
    "            for j in range(nb_actions[i]):\n",
    "                row = sample_actions.iloc[j]\n",
    "                # FORMAT ACTION\n",
    "                action = str(row.loc['item']) + '&' + str(row.loc['rating'])\n",
    "                sample_action.append(action)\n",
    "            \n",
    "            states.append(sample_state)\n",
    "            actions.append(sample_action)\n",
    "        \n",
    "        return states, actions\n",
    "\n",
    "    def gen_train_test(self, test_ratio, seed=None):\n",
    "        '''\n",
    "        Shuffle the historic of users and seperate it in a train and a test set.\n",
    "        Store the ids for each set.\n",
    "        An user can't be in both set.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        test_ratio:     float, ratio to control the sizes of the sets\n",
    "        seed:           float, seed on the shuffle\n",
    "        '''\n",
    "\n",
    "        n = len(self.histo)\n",
    "\n",
    "        if seed is not None:\n",
    "            random.Random(seed).shuffle(self.histo)\n",
    "        else:\n",
    "            random.shuffle(self.histo)\n",
    "\n",
    "        self.train = self.histo[:int((test_ratio * n))]\n",
    "        self.test = self.histo[int((test_ratio * n)):]\n",
    "        self.user_train = [h.iloc[0,0] for h in self.train]\n",
    "        print(self.user_train)\n",
    "        self.user_test = [h.iloc[0,0] for h in self.test]\n",
    "\n",
    "    def write_csv(self, filename, histo_to_write, delimiter=';', action_ratio=0.8, \n",
    "                  max_samp_by_user=5, max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
    "        '''\n",
    "        From a given historic, create a csv file with the format\n",
    "        Columns:        state, action_reward, n_state\n",
    "        Rows:           item&rating1 | item&rating2 | ...item&rating3 |... at filename location.\n",
    "\n",
    "        Paramters\n",
    "        ----------\n",
    "        filename:           string, path to the file to be produced\n",
    "        histo_to_write:     list(DataFrame), list of the historic for each user\n",
    "        delimiter:          string, optional delimiter for the csv\n",
    "        action_ratio:       float, optional ratio form which books in history will be selected\n",
    "        max_samp_by_user:   int, optional Number max of sample to make by user\n",
    "        max_state :         int, optional Number max of books to take for the 'state' column\n",
    "        max_action :        int, optional Number max of books to take for the 'action' action\n",
    "        nb_states :         array(int), optional Numbers of books to be taken for each sample made on user's historic\n",
    "        nb_actions :        array(int), optional Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        if given, size of nb_states is the number of sample by user sizes of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "        with open(filename, mode='w') as file:\n",
    "            f_writer = csv.writer(file, delimiter=delimiter)\n",
    "            f_writer.writerow(['state', 'action_reward', 'n_state'])\n",
    "            for user_histo in histo_to_write:\n",
    "                states, actions = self.sample_histo(user_histo, action_ratio, \n",
    "                                                    max_samp_by_user, max_state, max_action, nb_states, nb_actions)\n",
    "                for i in range(len(states)):\n",
    "                    # FORMAT STATE\n",
    "                    state_str = '|'.join(states[i])\n",
    "                    # FORMAT ACTION\n",
    "                    action_str = '|'.join(actions[i])\n",
    "                    # FORMAT N_STATE\n",
    "                    n_state_str = state_str + '|' + action_str\n",
    "                    f_writer.writerow([state_str, action_str, n_state_str])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'Books.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           item            user  rating   timestamp\n",
      "0    0001713353  A1C6M8LCIX4M6M     5.0  1123804800\n",
      "1    0001713353  A1REUF3A1YCPHM     5.0  1112140800\n",
      "2    0001713353   A1YRBRK2XM5D5     5.0  1081036800\n",
      "3    0001713353  A1V8ZR5P78P4ZU     5.0  1077321600\n",
      "4    0001713353  A2ZB06582NXCIV     5.0  1475452800\n",
      "..          ...             ...     ...         ...\n",
      "995  0001384198  A2WDPKL01ILCQ5     3.0  1390089600\n",
      "996  0001384198  A15WQ7V1OGJ2IE     5.0  1389744000\n",
      "997  0001384198  A3O11AZC86MP9E     5.0  1389398400\n",
      "998  0001384198   ARWYGWXB243RI     5.0  1389225600\n",
      "999  0001384198  A331AFVDOICK1E     3.0  1389052800\n",
      "\n",
      "[1000 rows x 4 columns]\n",
      "['0001384198', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0001932349', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001712799', '0002005263', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001713353', '0002005263', '0001713353', '0001713353', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '000047715X', '0001384198', '0002005263', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001713353', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0001061240', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '000047715X', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0002005263', '0001384198', '0002006448', '0001712799', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001932349', '000047715X', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001712799', '0001384198', '0002005263', '0001713353', '0001384198', '0001712799', '0001384198', '0001061240', '0001384198', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0002006448', '0002005263', '0001713353', '0001384198', '0001713353', '0002005263', '0001061240', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001713353', '0001384198', '0002005263', '0001932349', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001712799', '0002006448', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001712799', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001713353', '0002005263', '0001384198', '0001712799', '0001713353', '0001384198', '0001384198', '0001712799', '0001712799', '0001384198', '0001384198', '0001712799', '0001061240', '0002005263', '0002005263', '0002005263', '0001384198', '0002005263', '0002005263', '0001713353', '0001384198', '0001932349', '000047715X', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001061240', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '000047715X', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001712799', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001712799', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0002005263', '0001713353', '0001384198', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001713353', '0001384198', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001713353', '0001712799', '0001384198', '0002005263', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002006448', '0002005263', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001712799', '0001712799', '0001384198', '0002006448', '0001712799', '0002005263', '0001384198', '0001713353', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0001712799', '0001384198', '0001384198', '0001713353', '0001061240', '0001384198', '0001384198', '0002005263', '0002006448', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001932349', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001932349', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001713353', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001712799', '0002005263', '0001384198', '0001061240', '0001712799', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001712799', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002006448', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001061240', '0001384198', '0001061240', '0001384198', '0001384198', '0001061240', '0001713353', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001713353', '0002005263', '0001384198', '0002005263', '0002005263', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002006448', '0001712799', '0001384198', '0001061240', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0002005263', '0001061240', '0002005263', '0002005263', '000047715X', '0001384198', '0001713353', '0001384198', '0002005263', '0001384198', '0001713353', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001712799', '0001384198', '0002005263', '0001384198', '0001384198']\n",
      "797\n",
      "200\n",
      "train:  [         item            user  rating   timestamp\n",
      "0  0001384198  A14KFAV4FBJ4AJ     5.0  1420243200,          item           user  rating   timestamp\n",
      "0  0001384198  AKEJX9EQH9SW2     5.0  1487980800,          item            user  rating   timestamp\n",
      "0  0001384198  A2T0ZAD9IWYFB9     5.0  1414281600,          item           user  rating   timestamp\n",
      "0  0002005263  A7E2MSRVHAAO1     5.0  1054339200,          item            user  rating   timestamp\n",
      "0  0001713353  A30FZZMVK0N6CA     5.0  1258329600,          item           user  rating   timestamp\n",
      "0  0001384198  AW4ZFHCWEJM7P     5.0  1425081600,          item            user  rating   timestamp\n",
      "0  0001932349  A2NG0O7JGLBGUP     5.0  1063497600,          item            user  rating   timestamp\n",
      "0  0001384198  A1NKJW0TNRVS7O     5.0  1418428800,          item            user  rating   timestamp\n",
      "0  0002005263  A2R8RX05RDWD3K     5.0  1323216000,          item            user  rating   timestamp\n",
      "0  0001384198  A2RP0SJXBJMI1Q     4.0  1413331200]\n",
      "test: [         item            user  rating   timestamp\n",
      "0  0001384198  A27BP2ZOMVD59U     5.0  1419984000,          item            user  rating   timestamp\n",
      "0  0001384198  A3TEMOQZ2DQY07     5.0  1435795200,          item            user  rating   timestamp\n",
      "0  0001384198  A2JWKFFV8WVW16     2.0  1435017600,          item            user  rating   timestamp\n",
      "0  0001713353  A1V8ZR5P78P4ZU     5.0  1077321600,          item            user  rating   timestamp\n",
      "0  0001384198  A2UA54M3LDETU0     4.0  1412985600,          item            user  rating   timestamp\n",
      "0  0002005263  A2NKR6UJG80TZY     5.0  1467244800,          item            user  rating   timestamp\n",
      "0  0001384198  A3F6YRZGRQ5QGU     5.0  1502496000,          item            user  rating   timestamp\n",
      "0  0001384198  A16FAS8W58EPZW     5.0  1409011200,          item           user  rating   timestamp\n",
      "0  0001384198  ARWYGWXB243RI     5.0  1389225600,          item           user  rating   timestamp\n",
      "0  0001384198  A4U5Q0DNABCP8     5.0  1400025600]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "history_length = 12 # N in article\n",
    "ra_length = 4 # K in article\n",
    "discount_factor = 0.99 # Gamma in Bellman equation\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "tau = 0.001 # τ in Algorithm 3\n",
    "batch_size = 64\n",
    "nb_episodes = 100\n",
    "nb_rounds = 50\n",
    "filename_summary = 'summary.txt'\n",
    "alpha = 0.5 # α (alpha) in Equation (1)\n",
    "gamma = 0.9 # Γ (Gamma) in Equation (4)\n",
    "buffer_size = 1000000 # Size of replay memory D in article\n",
    "fixed_length = True # Fixed memory length\n",
    "\n",
    "\n",
    "dg = DataGenerator(datapath)\n",
    "dg.gen_train_test(0.8, seed=42)\n",
    "\n",
    "print(len(dg.train))\n",
    "print(len(dg.test))\n",
    "print('train: ', dg.train[:10])\n",
    "print('test:', dg.test[:10])\n",
    "\n",
    "#dg.write_csv('books_train.csv', dg.train, nb_states=[history_length], nb_actions=[ra_length])\n",
    "#dg.write_csv('books_test.csv', dg_test, nb_states=[history_length], nb_actions=[ra_length])\n",
    "\n",
    "#data = read_file('books_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Generator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int('003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsGenerator:\n",
    "    def __init__(self, train_users, data):\n",
    "        self.train_users = train_users\n",
    "        \n",
    "        # preprocess\n",
    "        self.data = data.sort_values(by=['timestamp'])\n",
    "        # make them start at 0\n",
    "        # 유저아이디 인트로 인덱싱해줘야함\n",
    "        \n",
    "        user_uniq = self.data['user'].unique()\n",
    "        user_ix = [num for num in range(len(user_uniq))]\n",
    "        #user_ix_uniq = dict(zip(user_ix, user_uniq))\n",
    "        user_uniq_ix = dict(zip(user_uniq, user_ix))\n",
    "        self.data.replace({\"user\": user_uniq_ix})\n",
    "    \n",
    "        item_uniq = self.data['item'].unique()\n",
    "        item_ix = [num for num in range(len(item_uniq))]\n",
    "        #user_ix_uniq = dict(zip(user_ix, user_uniq))\n",
    "        item_uniq_ix = dict(zip(item_uniq, item_ix))\n",
    "        self.data.replace({\"item\": item_uniq_ix})\n",
    "    \n",
    "        self.data['user'] = self.data['user'] \n",
    "        self.data['item'] = self.data['item'] \n",
    "        self.user_count = self.data['user'].max() + 1\n",
    "        self.book_count = self.data['item'].max() + 1\n",
    "        # list of rated books by each user\n",
    "        self.user_books = {}\n",
    "        \n",
    "        for user in range(self.user_count):\n",
    "            self.user_books[user] = self.data[self.data.user == user]['item'].tolist()\n",
    "        self.m = self.model()\n",
    "    \n",
    "    def model(self, hidden_layer_size=100):\n",
    "        m = Sequential()\n",
    "        m.add(Dense(hidden_layer_size, input_shape=(1, self.book_count)))\n",
    "        m.add(Dropout(0.2))\n",
    "        m.add(Dense(self.book_count, activation='softmax'))\n",
    "        m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return m\n",
    "    \n",
    "    def generate_input(self, user):\n",
    "        '''\n",
    "        Returns a context and a target for the user\n",
    "        \n",
    "        context: user's history with one random book removed\n",
    "        target: id of random removed book\n",
    "        '''\n",
    "        user_books_count = len(self.user_books[user])\n",
    "        # picking random book\n",
    "        random_index = np.random.randint(0, user_books_count - 1) # -1 avoids taking the las book\n",
    "        # setting target\n",
    "        target = np.zeros((1, self.book_count))\n",
    "        target[0][self.user_books[user][random_index]] = 1\n",
    "        # setting context\n",
    "        context = np.zeros((1, self.book_count))\n",
    "        context[0][self.user_books[user][:random_index] + self.user_books[user][random_index + 1:]] = 1\n",
    "        return context, target\n",
    "    \n",
    "    def train(self, nb_epochs=300, batch_size=10000):\n",
    "        '''\n",
    "        Trains the model from train_users's history\n",
    "        '''\n",
    "        for i in range(nb_epochs):\n",
    "            print('%d/%d' % (i+1, nb_epochs))\n",
    "            batch = [self.generate_input(user=np.random.choic(self.train_users) - 1) for _ in range(batch_size)]\n",
    "            X_train = np.array([b[0] for b in batch])\n",
    "            y_train = np.array([b[0] for b in batch])\n",
    "            self.m.fit(X_train, y_train, epochs=1, validation_split=0.5)\n",
    "        \n",
    "    def test(self, test_users, batch_size=10000):\n",
    "        '''\n",
    "        Returns [loss, accuracy] on the test set\n",
    "        '''\n",
    "        batch_test = [self.generate_input(user=np.random.choice(test_users) - 1) for _ in range(batch_size)]\n",
    "        X_test = np.array([b[0] for b in batch_test])\n",
    "        y_test = np.array([b[1] for b in batch_test])\n",
    "        return self.m.evaluate(X_test, y_test)\n",
    "    \n",
    "    def save_embeddings(self, file_name):\n",
    "        '''\n",
    "        Generates a csv file containing the vecotr embedding for each book\n",
    "        '''\n",
    "        inp = self.m.input                                          # input placeholder\n",
    "        outputs = [layer.output for layer in self.m.layers]         # all layer outputs\n",
    "        functor = K.function([inp, K.learning_phase()])             # evaluation function\n",
    "        \n",
    "        # append embeddings to vectors\n",
    "        vectors = []\n",
    "        for book_id in range(self.book_count):\n",
    "            book = np.zeros((1, 1, self.book_count))\n",
    "            book[0][0][book_id] = 1\n",
    "            layer_outs = fuctor([book])\n",
    "            vector = [str(v) for v in layer_outs[0][0][0]]\n",
    "            vector = '|'.join(vector)\n",
    "            vectors.append([book_id, vector])\n",
    "        \n",
    "        # saves as a csv file\n",
    "        embeddings = pd.DataFrame(vectors, columns=['item_id', 'vectors']).astype({'book_id': 'int32'})\n",
    "        embeddings.to_csv(file_name, sep=';', index=False)\n",
    "        files.download(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, item_embeddings):\n",
    "        self.item_embeddings = item_embeddings\n",
    "    \n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "    \n",
    "    def get_embedding(self, item_index):\n",
    "        return self.item_embeddings[item_index]\n",
    "    \n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embeddiing(item) for item in item_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data_path):\n",
    "    '''\n",
    "    Load data from train.csv or test.csv\n",
    "    '''\n",
    "    data = pd.read_csv(data_path, sep=';')\n",
    "    for col in ['state', 'n_state', 'action_reward']:\n",
    "        data[col] = [np.array([[np.int(k) for k in ee.split('&')] for ee in e.split('|')]) for e in data[col]]\n",
    "    for col in ['state', 'n_state']:\n",
    "        data[col] = [np.array([e[0] for e in l]) for l in data[col]]\n",
    "    \n",
    "    data['action'] = [[e[0] for e in l] for l in data['action_reward']]\n",
    "    data['reward'] = [tuple(e[1] for e in l) for l in data['action_reward']]\n",
    "    data.drop(columns=['action_reward'], inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    '''\n",
    "    Load embeddings (a vector for each item)\n",
    "    '''\n",
    "    embeddings = pd.read_csv(embeddings_path, sep=';')\n",
    "    \n",
    "    return np.array([[np.float64(k) for k in e.split('|')] for e in embeddings['vectors']])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = EmbeddingsGenerator(dg.user_train, pd.read_csv('Books.csv', sep='\\t', names=['user', 'item', 'rating', 'timestamp']))\n",
    "eg.train(np_epochs=300)\n",
    "train_loss, train_accuracy = eg.test(df.user_train)\n",
    "print('Train set: Loss=%.4f ; Accuracy=%.1f%%' % (train_loss, train_accuracy * 100))\n",
    "test_loss, test_accuracy = eg.test(dg.user_test)\n",
    "print('Test set; Loss=%.4f; Accuracy=%.1f%%' % (test_loss, test_accuracy * 100))\n",
    "eg.save_embeddings('embeddings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

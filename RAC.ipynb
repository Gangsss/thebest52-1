{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, datapath):\n",
    "        ''' \n",
    "        Load data from the DB Books\n",
    "        List the users and items\n",
    "        List all the users historic\n",
    "        '''\n",
    "        self.data = self.load_datas(datapath)\n",
    "        self.users = self.data['user'].unique()\n",
    "        self.items = self.data['item'].unique()\n",
    "        self.histo = self.gen_histo()\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "        \n",
    "\n",
    "    def load_datas(self, datapath):\n",
    "        '''\n",
    "        Load the data and merge the name of each books\n",
    "        A row corresponds to a rate given by a user to books\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapath:   string, path to the data books contain user, item, rating, timestamp\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result:     DataFram, contains all the ratings\n",
    "        '''\n",
    "        data = pd.read_csv(datapath, names=['item', 'user', 'rating', 'timestamp'])\n",
    "        data = data.sort_values([\"item\"], ascending=[False])\n",
    "        data = data[:100]\n",
    "        print(data)\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def gen_histo(self):\n",
    "        '''\n",
    "        Group all rates given by users and store them from older to most recent\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result:     List(DataFrame), List of the historic for each user\n",
    "        '''\n",
    "        historic_users = []\n",
    "        for i, u in enumerate(self.users):\n",
    "            temp = self.data[self.data['user'] == u]\n",
    "            temp = temp.sort_values('timestamp').reset_index()\n",
    "            temp.drop('index', axis=1, inplace=True)\n",
    "            historic_users.append(temp)\n",
    "        return historic_users\n",
    "    \n",
    "    def sample_histo(self, user_histo, action_ratio=0.8, \n",
    "                     max_samp_by_user=5, max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
    "        '''\n",
    "        action이 이 코드에 따르면 각 아이템에 대한 rating이 action인 것으로 보이는데 book이어야 하는 것아닌가?\n",
    "        '''\n",
    "        '''\n",
    "        For a given historic, make one or multiple sampling.\n",
    "        If no optional argument given for nb_states and nb_actions, \n",
    "        then the sampling is random and each sample can have differents size for action and state.\n",
    "        To normalize sampling we need to give list of the numbers of states and actions to be sampled\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_histo:         DataFrame, historic of user\n",
    "        delimiter:          float, optional delimiter for the csv\n",
    "        action_ratio:       float, optional ratio form which books in history will be selected\n",
    "        max_samp_by_user:   int, optional Number max of sample to make by user\n",
    "        max_state:          int, optional Number max of books to take for the 'state' column\n",
    "        max_action:         int, optional Number max of books to take for the 'action' column\n",
    "        nb_state:           array(int), optional Numbers of books to be taken for each sample made on user's historic\n",
    "        nb_actions:         array(int), optional Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states:             List(String), All the states sampled, format of a sample: item & rating\n",
    "        actions:            List(String), All the actions sampled, format of a sample: item & rating\n",
    "\n",
    "        Notes\n",
    "        -------\n",
    "        States must be before(timestamp) the actions.\n",
    "        If given, size of nb_states is the number of sample by user size of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "\n",
    "        n = len(user_histo)\n",
    "        print('length of user_histo', n)\n",
    "        print('user_histo', user_histo)\n",
    "        \n",
    "        sep = int(action_ratio * n)\n",
    "        nb_sample = random.randint(1, max_samp_by_user)\n",
    "        if not nb_states:\n",
    "            nb_states = [min(random.randint(1, sep), max_state) for i in range(nb_sample)]\n",
    "        if not nb_actions:\n",
    "            nb_actions = [min(random.randint(1, n-sep), max_action) for i in range(nb_sample)]\n",
    "        \n",
    "        assert len(nb_states) == len(nb_actions)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "        # SELECT SAMPLES IN HISTO\n",
    "        for i in range(len(nb_states)):\n",
    "            \n",
    "            sample_states = user_histo.iloc[0:sep].sample(nb_states[i])\n",
    "            sample_actions = user_histo.iloc[-(n-sep):].sample(nb_actions[i])\n",
    "\n",
    "                \n",
    "            sample_state = []\n",
    "            sample_action = []\n",
    "            for j in range(nb_states[i]):\n",
    "                row = sample_states.iloc[j]\n",
    "                # FORMAT STATE\n",
    "                state = str(row.loc['item']) + '&' + str(row.loc['rating'])\n",
    "                sample_state.append(state)\n",
    "            \n",
    "            for j in range(nb_actions[i]):\n",
    "                row = sample_actions.iloc[j]\n",
    "                # FORMAT ACTION\n",
    "                action = str(row.loc['item']) + '&' + str(row.loc['rating'])\n",
    "                sample_action.append(action)\n",
    "            \n",
    "            states.append(sample_state)\n",
    "            actions.append(sample_action)\n",
    "        \n",
    "        return states, actions\n",
    "\n",
    "    def gen_train_test(self, test_ratio, seed=None):\n",
    "        '''\n",
    "        Shuffle the historic of users and seperate it in a train and a test set.\n",
    "        Store the ids for each set.\n",
    "        An user can't be in both set.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        test_ratio:     float, ratio to control the sizes of the sets\n",
    "        seed:           float, seed on the shuffle\n",
    "        '''\n",
    "\n",
    "        n = len(self.histo)\n",
    "\n",
    "        if seed is not None:\n",
    "            random.Random(seed).shuffle(self.histo)\n",
    "        else:\n",
    "            random.shuffle(self.histo)\n",
    "\n",
    "        self.train = self.histo[:int((test_ratio * n))]\n",
    "        self.test = self.histo[int((test_ratio * n)):]\n",
    "        self.user_train = [h.iloc[0,0] for h in self.train]\n",
    "        self.user_test = [h.iloc[0,0] for h in self.test]\n",
    "\n",
    "    def write_csv(self, filename, histo_to_write, delimiter=';', action_ratio=0.8, \n",
    "                  max_samp_by_user=5, max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
    "        '''\n",
    "        From a given historic, create a csv file with the format\n",
    "        Columns:        state, action_reward, n_state\n",
    "        Rows:           item&rating1 | item&rating2 | ...item&rating3 |... at filename location.\n",
    "\n",
    "        Paramters\n",
    "        ----------\n",
    "        filename:           string, path to the file to be produced\n",
    "        histo_to_write:     list(DataFrame), list of the historic for each user\n",
    "        delimiter:          string, optional delimiter for the csv\n",
    "        action_ratio:       float, optional ratio form which books in history will be selected\n",
    "        max_samp_by_user:   int, optional Number max of sample to make by user\n",
    "        max_state :         int, optional Number max of books to take for the 'state' column\n",
    "        max_action :        int, optional Number max of books to take for the 'action' action\n",
    "        nb_states :         array(int), optional Numbers of books to be taken for each sample made on user's historic\n",
    "        nb_actions :        array(int), optional Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        if given, size of nb_states is the number of sample by user sizes of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "        with open(filename, mode='w') as file:\n",
    "            f_writer = csv.writer(file, delimiter=delimiter)\n",
    "            f_writer.writerow(['state', 'action_reward', 'n_state'])\n",
    "            for user_histo in histo_to_write:\n",
    "                states, actions = self.sample_histo(user_histo, action_ratio, \n",
    "                                                    max_samp_by_user, max_state, max_action, nb_states, nb_actions)\n",
    "                for i in range(len(states)):\n",
    "                    # FORMAT STATE\n",
    "                    state_str = '|'.join(states[i])\n",
    "                    # FORMAT ACTION\n",
    "                    action_str = '|'.join(actions[i])\n",
    "                    # FORMAT N_STATE\n",
    "                    n_state_str = state_str + '|' + action_str\n",
    "                    f_writer.writerow([state_str, action_str, n_state_str])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'Books.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                item            user  rating   timestamp\n",
      "51311620  B01HJFHYMA   A7J9KA8SAL0MA     5.0  1473120000\n",
      "51311619  B01HJEB422  A2TO93KMH3DJIK     5.0  1485648000\n",
      "51311618  B01HJEB422   AB9M1MQPBAS2J     5.0  1487030400\n",
      "51311617  B01HJDS76Y  A26Q9T9G9MTX9S     1.0  1467158400\n",
      "51311616  B01HJDS76Y  A3P8PUZFHWFK1E     4.0  1467244800\n",
      "...              ...             ...     ...         ...\n",
      "37990237  B01HJ4GFPS  A29ZNC4AM7NL4S     5.0  1466812800\n",
      "37990236  B01HJ4GFPS  A3ONPQMO4POB2P     5.0  1466812800\n",
      "37990235  B01HJ4GFPS   AIK7QH38LLGP6     5.0  1466812800\n",
      "37990234  B01HJ4GFPS  A3CNKAPXM2GRXT     5.0  1466812800\n",
      "37990233  B01HJ4GFPS  A3DOHMP948TG9N     5.0  1466812800\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "78\n",
      "20\n",
      "train:  [         item            user  rating   timestamp\n",
      "0  B01HJ4GFPS  A1QDUNLE8SAQAL     5.0  1523318400,          item            user  rating   timestamp\n",
      "0  B01HJ56VC4  A3KI93AT8FAXI1     5.0  1488758400,          item            user  rating   timestamp\n",
      "0  B01HJ56VC4  A1L9JKDKE8FHB8     5.0  1480464000,          item            user  rating   timestamp\n",
      "0  B01HJBZV74  A2D2545LW6NUSE     1.0  1480032000,          item            user  rating   timestamp\n",
      "0  B01HJ8KWDU  A2NL89Q96DCJGH     3.0  1467676800,          item            user  rating   timestamp\n",
      "0  B01HJ8KWDU  A3AS0V5XE7X0D7     5.0  1467676800,          item            user  rating   timestamp\n",
      "0  B01HJEB422  A2TO93KMH3DJIK     5.0  1485648000,          item           user  rating   timestamp\n",
      "0  B01HJ56VC4  ARU2EYSBGQYNW     5.0  1491264000,          item            user  rating   timestamp\n",
      "0  B01HJBPTUI  A2K8VPB4UUNSQR     5.0  1467590400,          item            user  rating   timestamp\n",
      "0  B01HJ56VC4  A3GSHMATNCP0XP     5.0  1487721600]\n",
      "test: [         item           user  rating   timestamp\n",
      "0  B01HJ56VC4  AP67X89L98Q9I     5.0  1493078400,          item            user  rating   timestamp\n",
      "0  B01HJ8KWDU  A2XN8BA8TRGCVK     5.0  1468022400,          item            user  rating   timestamp\n",
      "0  B01HJ8KWDU  A2KJWN2ZVPO5IB     5.0  1466899200,          item            user  rating   timestamp\n",
      "0  B01HJ4GFPS  A14OH5PO11YY17     5.0  1472774400,          item           user  rating   timestamp\n",
      "0  B01HJ4GFPS  AIK7QH38LLGP6     5.0  1466812800,          item            user  rating   timestamp\n",
      "0  B01HJDS76Y  A3P8PUZFHWFK1E     4.0  1467244800,          item            user  rating   timestamp\n",
      "0  B01HJ56VC4  A19J4QX031TXZM     4.0  1490572800,          item            user  rating   timestamp\n",
      "0  B01HJ56VC4  A3IVDYDQPHIE3A     5.0  1498089600,          item            user  rating   timestamp\n",
      "0  B01HJBPTUI  A3FWFK4UNUHY4X     5.0  1469232000,          item           user  rating   timestamp\n",
      "0  B01HJ56VC4  AXWWHHB8NNQZH     4.0  1491177600]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "history_length = 12 # N in article\n",
    "ra_length = 4 # K in article\n",
    "discount_factor = 0.99 # Gamma in Bellman equation\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "tau = 0.001 # τ in Algorithm 3\n",
    "batch_size = 64\n",
    "nb_episodes = 100\n",
    "nb_rounds = 50\n",
    "filename_summary = 'summary.txt'\n",
    "alpha = 0.5 # α (alpha) in Equation (1)\n",
    "gamma = 0.9 # Γ (Gamma) in Equation (4)\n",
    "buffer_size = 1000000 # Size of replay memory D in article\n",
    "fixed_length = True # Fixed memory length\n",
    "\n",
    "\n",
    "dg = DataGenerator(datapath)\n",
    "dg.gen_train_test(0.8, seed=42)\n",
    "\n",
    "print(len(dg.train))\n",
    "print(len(dg.test))\n",
    "print('train: ', dg.train[:10])\n",
    "print('test:', dg.test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of user_histo 1\n",
      "user_histo          item           user  rating   timestamp\n",
      "0  0768442117  AZZZJUUS9K2N2     5.0  1370822400\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e292f2959483>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'books_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mra_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'books_test.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mhistory_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_actions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mra_length\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'books_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9b5a266ec2ae>\u001b[0m in \u001b[0;36mwrite_csv\u001b[0;34m(self, filename, histo_to_write, delimiter, action_ratio, max_samp_by_user, max_state, max_action, nb_states, nb_actions)\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0muser_histo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhisto_to_write\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 states, actions = self.sample_histo(user_histo, action_ratio, \n\u001b[0;32m--> 184\u001b[0;31m                                                     max_samp_by_user, max_state, max_action, nb_states, nb_actions)\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0;31m# FORMAT STATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9b5a266ec2ae>\u001b[0m in \u001b[0;36msample_histo\u001b[0;34m(self, user_histo, action_ratio, max_samp_by_user, max_state, max_action, nb_states, nb_actions)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0msample_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m                 \u001b[0;31m# FORMAT STATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'&'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rating'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2137\u001b[0m             \u001b[0;31m# validate the location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2138\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2061\u001b[0m         \u001b[0mlen_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen_axis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlen_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "dg.write_csv('books_train.csv', dg.train, nb_states=[history_length], nb_actions=[ra_length])\n",
    "dg.write_csv('books_test.csv', dg.test, nb_states=[history_length], nb_actions=[ra_length])\n",
    "\n",
    "data = read_file('books_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings Generator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsGenerator:\n",
    "    def __init__(self, train_users, data):\n",
    "        self.train_users = train_users\n",
    "        \n",
    "        # preprocess\n",
    "        self.data = data.sort_values(by=['timestamp'])\n",
    "        # make them start at 0\n",
    "        # 유저아이디 인트로 인덱싱해줘야함\n",
    "        \n",
    "        user_uniq = self.data['user'].unique()\n",
    "        user_ix = [num for num in range(len(user_uniq))]\n",
    "        #user_ix_uniq = dict(zip(user_ix, user_uniq))\n",
    "        user_uniq_ix = dict(zip(user_uniq, user_ix))\n",
    "        self.data.replace({\"user\": user_uniq_ix})\n",
    "    \n",
    "        item_uniq = self.data['item'].unique()\n",
    "        item_ix = [num for num in range(len(item_uniq))]\n",
    "        #user_ix_uniq = dict(zip(user_ix, user_uniq))\n",
    "        item_uniq_ix = dict(zip(item_uniq, item_ix))\n",
    "        self.data.replace({\"item\": item_uniq_ix})\n",
    "    \n",
    "        self.data['user'] = self.data['user'] \n",
    "        self.data['item'] = self.data['item'] \n",
    "        self.user_count = self.data['user'].max() + 1\n",
    "        self.book_count = self.data['item'].max() + 1\n",
    "        # list of rated books by each user\n",
    "        self.user_books = {}\n",
    "        \n",
    "        for user in range(self.user_count):\n",
    "            self.user_books[user] = self.data[self.data.user == user]['item'].tolist()\n",
    "        self.m = self.model()\n",
    "    \n",
    "    def model(self, hidden_layer_size=100):\n",
    "        m = Sequential()\n",
    "        m.add(Dense(hidden_layer_size, input_shape=(1, self.book_count)))\n",
    "        m.add(Dropout(0.2))\n",
    "        m.add(Dense(self.book_count, activation='softmax'))\n",
    "        m.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return m\n",
    "    \n",
    "    def generate_input(self, user):\n",
    "        '''\n",
    "        Returns a context and a target for the user\n",
    "        \n",
    "        context: user's history with one random book removed\n",
    "        target: id of random removed book\n",
    "        '''\n",
    "        user_books_count = len(self.user_books[user])\n",
    "        # picking random book\n",
    "        random_index = np.random.randint(0, user_books_count - 1) # -1 avoids taking the las book\n",
    "        # setting target\n",
    "        target = np.zeros((1, self.book_count))\n",
    "        target[0][self.user_books[user][random_index]] = 1\n",
    "        # setting context\n",
    "        context = np.zeros((1, self.book_count))\n",
    "        context[0][self.user_books[user][:random_index] + self.user_books[user][random_index + 1:]] = 1\n",
    "        return context, target\n",
    "    \n",
    "    def train(self, nb_epochs=300, batch_size=10000):\n",
    "        '''\n",
    "        Trains the model from train_users's history\n",
    "        '''\n",
    "        for i in range(nb_epochs):\n",
    "            print('%d/%d' % (i+1, nb_epochs))\n",
    "            batch = [self.generate_input(user=np.random.choic(self.train_users) - 1) for _ in range(batch_size)]\n",
    "            X_train = np.array([b[0] for b in batch])\n",
    "            y_train = np.array([b[0] for b in batch])\n",
    "            self.m.fit(X_train, y_train, epochs=1, validation_split=0.5)\n",
    "        \n",
    "    def test(self, test_users, batch_size=10000):\n",
    "        '''\n",
    "        Returns [loss, accuracy] on the test set\n",
    "        '''\n",
    "        batch_test = [self.generate_input(user=np.random.choice(test_users) - 1) for _ in range(batch_size)]\n",
    "        X_test = np.array([b[0] for b in batch_test])\n",
    "        y_test = np.array([b[1] for b in batch_test])\n",
    "        return self.m.evaluate(X_test, y_test)\n",
    "    \n",
    "    def save_embeddings(self, file_name):\n",
    "        '''\n",
    "        Generates a csv file containing the vecotr embedding for each book\n",
    "        '''\n",
    "        inp = self.m.input                                          # input placeholder\n",
    "        outputs = [layer.output for layer in self.m.layers]         # all layer outputs\n",
    "        functor = K.function([inp, K.learning_phase()])             # evaluation function\n",
    "        \n",
    "        # append embeddings to vectors\n",
    "        vectors = []\n",
    "        for book_id in range(self.book_count):\n",
    "            book = np.zeros((1, 1, self.book_count))\n",
    "            book[0][0][book_id] = 1\n",
    "            layer_outs = fuctor([book])\n",
    "            vector = [str(v) for v in layer_outs[0][0][0]]\n",
    "            vector = '|'.join(vector)\n",
    "            vectors.append([book_id, vector])\n",
    "        \n",
    "        # saves as a csv file\n",
    "        embeddings = pd.DataFrame(vectors, columns=['item_id', 'vectors']).astype({'book_id': 'int32'})\n",
    "        embeddings.to_csv(file_name, sep=';', index=False)\n",
    "        files.download(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    def __init__(self, item_embeddings):\n",
    "        self.item_embeddings = item_embeddings\n",
    "    \n",
    "    def size(self):\n",
    "        return self.item_embeddings.shape[1]\n",
    "\n",
    "    def get_embedding_vector(self):\n",
    "        return self.item_embeddings\n",
    "    \n",
    "    def get_embedding(self, item_index):\n",
    "        return self.item_embeddings[item_index]\n",
    "    \n",
    "    def embed(self, item_list):\n",
    "        return np.array([self.get_embeddiing(item) for item in item_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(data_path):\n",
    "    '''\n",
    "    Load data from train.csv or test.csv\n",
    "    '''\n",
    "    data = pd.read_csv(data_path, sep=';')\n",
    "    for col in ['state', 'n_state', 'action_reward']:\n",
    "        data[col] = [np.array([[np.int(k) for k in ee.split('&')] for ee in e.split('|')]) for e in data[col]]\n",
    "    for col in ['state', 'n_state']:\n",
    "        data[col] = [np.array([e[0] for e in l]) for l in data[col]]\n",
    "    \n",
    "    data['action'] = [[e[0] for e in l] for l in data['action_reward']]\n",
    "    data['reward'] = [tuple(e[1] for e in l) for l in data['action_reward']]\n",
    "    data.drop(columns=['action_reward'], inplace=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def read_embeddings(embeddings_path):\n",
    "    '''\n",
    "    Load embeddings (a vector for each item)\n",
    "    '''\n",
    "    embeddings = pd.read_csv(embeddings_path, sep=';')\n",
    "    \n",
    "    return np.array([[np.float64(k) for k in e.split('|')] for e in embeddings['vectors']])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg = EmbeddingsGenerator(dg.user_train, pd.read_csv('Books.csv', sep='\\t', names=['user', 'item', 'rating', 'timestamp']))\n",
    "eg.train(np_epochs=300)\n",
    "train_loss, train_accuracy = eg.test(df.user_train)\n",
    "print('Train set: Loss=%.4f ; Accuracy=%.1f%%' % (train_loss, train_accuracy * 100))\n",
    "test_loss, test_accuracy = eg.test(dg.user_test)\n",
    "print('Test set; Loss=%.4f; Accuracy=%.1f%%' % (test_loss, test_accuracy * 100))\n",
    "eg.save_embeddings('embeddings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

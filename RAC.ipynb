{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.backend as K\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataGenerator\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, datapath):\n",
    "        ''' \n",
    "        Load data from the DB Books\n",
    "        List the users and items\n",
    "        List all the users historic\n",
    "        '''\n",
    "        self.data = self.load_datas(datapath)\n",
    "        self.users = self.data['user'].unique()\n",
    "        self.items = self.data['item'].unique()\n",
    "        self.histo = self.gen_histo()\n",
    "        self.train = []\n",
    "        self.test = []\n",
    "\n",
    "    def load_datas(self, datapath):\n",
    "        '''\n",
    "        Load the data and merge the name of each books\n",
    "        A row corresponds to a rate given by a user to books\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        datapath:   string, path to the data books contain user, item, rating, timestamp\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result:     DataFram, contains all the ratings\n",
    "        '''\n",
    "        data = pd.read_csv(datapath, names=['item', 'user', 'rating', 'timestamp'])\n",
    "        data = data[:1000]\n",
    "\n",
    "        return data\n",
    "    \n",
    "    def gen_histo(self):\n",
    "        '''\n",
    "        Group all rates given by users and store them from older to most recent\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        result:     List(DataFrame), List of the historic for each user\n",
    "        '''\n",
    "        historic_user = []\n",
    "        for i, u in enumerate(self.users):\n",
    "            temp = self.data[self.data['user'] == u]\n",
    "            temp = temp.sort_values('timestamp').reset_index()\n",
    "            temp.drop('index', axis=1, inplace=True)\n",
    "            historic_user.append(temp)\n",
    "        return historic_user\n",
    "    \n",
    "    def sample_histo(self, user_histo, action_ratio=0.8, \n",
    "                     max_samp_by_user=5, max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
    "        '''\n",
    "        For a given historic, make one or multiple sampling.\n",
    "        If no optional argument given for nb_states and nb_actions, \n",
    "        then the sampling is random and each sample can have differents size for action and state.\n",
    "        To normalize sampling we need to give list of the numbers of states and actions to be sampled\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_histo:         DataFrame, historic of user\n",
    "        delimiter:          float, optional delimiter for the csv\n",
    "        action_ratio:       float, optional ratio form which books in history will be selected\n",
    "        max_samp_by_user:   int, optional Number max of sample to make by user\n",
    "        max_state:          int, optional Number max of books to take for the 'state' column\n",
    "        max_action:         int, optional Number max of books to take for the 'action' column\n",
    "        nb_state:           array(int), optional Numbers of books to be taken for each sample made on user's historic\n",
    "        nb_actions:         array(int), optional Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        states:             List(String), All the states sampled, format of a sample: item & rating\n",
    "        actions:            List(String), All the actions sampled, format of a sample: item & rating\n",
    "\n",
    "        Notes\n",
    "        -------\n",
    "        States must be before(timestamp) the actions.\n",
    "        If given, size of nb_states is the number of sample by user size of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "\n",
    "        n = len(user_histo)\n",
    "        print(n)\n",
    "        sep = int(action_ratio * n)\n",
    "        nb_sample = random.randint(1, max_samp_by_user)\n",
    "        if not nb_states:\n",
    "            nb_states = [min(random.randint(1, sep), max_state) for i in range(nb_sample)]\n",
    "        if not nb_actions:\n",
    "            nb_actions = [min(random.randint(1, n-sep), max_action) for i in range(nb_sample)]\n",
    "        \n",
    "        assert len(nb_states) == len(nb_actions)\n",
    "\n",
    "        states = []\n",
    "        actions = []\n",
    "\n",
    "        # SELECT SAMPLES IN HISTO\n",
    "        for i in range(len(nb_states)):\n",
    "            sample_states = user_histo.iloc[0:sep].sample(nb_states[i])\n",
    "            sample_actions = user_histo.iloc[-(n-sep):].sample(nb_actions[i])\n",
    "\n",
    "            sample_state = []\n",
    "            sample_action = []\n",
    "            for j in range(nb_states[i]):\n",
    "                row = sample_states.iloc[j]\n",
    "                # FORMAT STATE\n",
    "                state = str(row.loc['item']) + '&' + str(row.loc['rating'])\n",
    "                sample_state.append(state)\n",
    "            \n",
    "            for j in range(nb_actions[i]):\n",
    "                row = sample_actions.iloc[j]\n",
    "                # FORMAT ACTION\n",
    "                action = str(row.loc['item']) + '&' + str(row.loc['rating'])\n",
    "                sample_action.append(action)\n",
    "            \n",
    "            states.append(sample_state)\n",
    "            actions.append(sample_action)\n",
    "        \n",
    "        return states, actions\n",
    "\n",
    "    def gen_train_test(self, test_ratio, seed=None):\n",
    "        '''\n",
    "        Shuffle the historic of users and seperate it in a train and a test set.\n",
    "        Store the ids for each set.\n",
    "        An user can't be in both set.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        test_ratio:     float, ratio to control the sizes of the sets\n",
    "        seed:           float, seed on the shuffle\n",
    "        '''\n",
    "\n",
    "        n = len(self.histo)\n",
    "\n",
    "        if seed is not None:\n",
    "            random.Random(seed).shuffle(self.histo)\n",
    "        else:\n",
    "            random.shuffle(self.histo)\n",
    "\n",
    "        self.train = self.histo[:int((test_ratio * n))]\n",
    "        self.test = self.histo[int((test_ratio * n)):]\n",
    "        self.user_train = [h.iloc[0,0] for h in self.train]\n",
    "        print(self.user_train)\n",
    "        self.user_test = [h.iloc[0,0] for h in self.test]\n",
    "\n",
    "    def write_csv(self, filename, histo_to_write, delimiter=';', action_ratio=0.8, \n",
    "                  max_samp_by_user=5, max_state=100, max_action=50, nb_states=[], nb_actions=[]):\n",
    "        '''\n",
    "        From a given historic, create a csv file with the format\n",
    "        Columns:        state, action_reward, n_state\n",
    "        Rows:           item&rating1 | item&rating2 | ...item&rating3 |... at filename location.\n",
    "\n",
    "        Paramters\n",
    "        ----------\n",
    "        filename:           string, path to the file to be produced\n",
    "        histo_to_write:     list(DataFrame), list of the historic for each user\n",
    "        delimiter:          string, optional delimiter for the csv\n",
    "        action_ratio:       float, optional ratio form which books in history will be selected\n",
    "        max_samp_by_user:   int, optional Number max of sample to make by user\n",
    "        max_state :         int, optional Number max of books to take for the 'state' column\n",
    "        max_action :        int, optional Number max of books to take for the 'action' action\n",
    "        nb_states :         array(int), optional Numbers of books to be taken for each sample made on user's historic\n",
    "        nb_actions :        array(int), optional Numbers of rating to be taken for each sample made on user's historic\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        if given, size of nb_states is the number of sample by user sizes of nb_states and nb_actions must be equals\n",
    "        '''\n",
    "        with open(filename, mode='w') as file:\n",
    "            f_writer = csv.writer(file, delimiter=delimiter)\n",
    "            f_writer.writerow(['state', 'action_reward', 'n_state'])\n",
    "            for user_histo in histo_to_write:\n",
    "                states, actions = self.sample_histo(user_histo, action_ratio, \n",
    "                                                    max_samp_by_user, max_state, max_action, nb_states, nb_actions)\n",
    "                for i in range(len(states)):\n",
    "                    # FORMAT STATE\n",
    "                    state_str = '|'.join(states[i])\n",
    "                    # FORMAT ACTION\n",
    "                    action_str = '|'.join(actions[i])\n",
    "                    # FORMAT N_STATE\n",
    "                    n_state_str = state_str + '|' + action_str\n",
    "                    f_writer.writerow([state_str, action_str, n_state_str])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = 'Books.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0001384198', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0001932349', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001712799', '0002005263', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001713353', '0002005263', '0001713353', '0001713353', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '000047715X', '0001384198', '0002005263', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001713353', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0001061240', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '000047715X', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0002005263', '0001384198', '0002006448', '0001712799', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001932349', '000047715X', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001712799', '0001384198', '0002005263', '0001713353', '0001384198', '0001712799', '0001384198', '0001061240', '0001384198', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0002006448', '0002005263', '0001713353', '0001384198', '0001713353', '0002005263', '0001061240', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001713353', '0001384198', '0002005263', '0001932349', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001712799', '0002006448', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001712799', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001713353', '0002005263', '0001384198', '0001712799', '0001713353', '0001384198', '0001384198', '0001712799', '0001712799', '0001384198', '0001384198', '0001712799', '0001061240', '0002005263', '0002005263', '0002005263', '0001384198', '0002005263', '0002005263', '0001713353', '0001384198', '0001932349', '000047715X', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001061240', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '000047715X', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001712799', '0002005263', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001712799', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0002005263', '0001713353', '0001384198', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001713353', '0001384198', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001713353', '0001712799', '0001384198', '0002005263', '0001384198', '0001713353', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002006448', '0002005263', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0002005263', '0002005263', '0002005263', '0001384198', '0001384198', '0001712799', '0001712799', '0001384198', '0002006448', '0001712799', '0002005263', '0001384198', '0001713353', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0001712799', '0001384198', '0001384198', '0001713353', '0001061240', '0001384198', '0001384198', '0002005263', '0002006448', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001932349', '0001713353', '0001384198', '0001384198', '0001384198', '0001384198', '0001712799', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001932349', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001713353', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001713353', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0001061240', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001712799', '0002005263', '0001384198', '0001061240', '0001712799', '0001384198', '0001384198', '0001384198', '0001061240', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001712799', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002006448', '0001384198', '0002005263', '0001384198', '0001384198', '0002005263', '0002005263', '0001384198', '0001384198', '0002005263', '0001384198', '0001061240', '0001384198', '0001061240', '0001384198', '0001384198', '0001061240', '0001713353', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001713353', '0002005263', '0001384198', '0002005263', '0002005263', '0001712799', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0002006448', '0001712799', '0001384198', '0001061240', '0001384198', '0001384198', '0001712799', '0001384198', '0001384198', '0002005263', '0001061240', '0002005263', '0002005263', '000047715X', '0001384198', '0001713353', '0001384198', '0002005263', '0001384198', '0001713353', '0001384198', '0001384198', '0002005263', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0001384198', '0002005263', '0001384198', '0001712799', '0001384198', '0002005263', '0001384198', '0001384198']\n",
      "797\n",
      "200\n",
      "train:  [         item            user  rating   timestamp\n",
      "0  0001384198  A14KFAV4FBJ4AJ     5.0  1420243200,          item           user  rating   timestamp\n",
      "0  0001384198  AKEJX9EQH9SW2     5.0  1487980800,          item            user  rating   timestamp\n",
      "0  0001384198  A2T0ZAD9IWYFB9     5.0  1414281600,          item           user  rating   timestamp\n",
      "0  0002005263  A7E2MSRVHAAO1     5.0  1054339200,          item            user  rating   timestamp\n",
      "0  0001713353  A30FZZMVK0N6CA     5.0  1258329600,          item           user  rating   timestamp\n",
      "0  0001384198  AW4ZFHCWEJM7P     5.0  1425081600,          item            user  rating   timestamp\n",
      "0  0001932349  A2NG0O7JGLBGUP     5.0  1063497600,          item            user  rating   timestamp\n",
      "0  0001384198  A1NKJW0TNRVS7O     5.0  1418428800,          item            user  rating   timestamp\n",
      "0  0002005263  A2R8RX05RDWD3K     5.0  1323216000,          item            user  rating   timestamp\n",
      "0  0001384198  A2RP0SJXBJMI1Q     4.0  1413331200]\n",
      "test: [         item            user  rating   timestamp\n",
      "0  0001384198  A27BP2ZOMVD59U     5.0  1419984000,          item            user  rating   timestamp\n",
      "0  0001384198  A3TEMOQZ2DQY07     5.0  1435795200,          item            user  rating   timestamp\n",
      "0  0001384198  A2JWKFFV8WVW16     2.0  1435017600,          item            user  rating   timestamp\n",
      "0  0001713353  A1V8ZR5P78P4ZU     5.0  1077321600,          item            user  rating   timestamp\n",
      "0  0001384198  A2UA54M3LDETU0     4.0  1412985600,          item            user  rating   timestamp\n",
      "0  0002005263  A2NKR6UJG80TZY     5.0  1467244800,          item            user  rating   timestamp\n",
      "0  0001384198  A3F6YRZGRQ5QGU     5.0  1502496000,          item            user  rating   timestamp\n",
      "0  0001384198  A16FAS8W58EPZW     5.0  1409011200,          item           user  rating   timestamp\n",
      "0  0001384198  ARWYGWXB243RI     5.0  1389225600,          item           user  rating   timestamp\n",
      "0  0001384198  A4U5Q0DNABCP8     5.0  1400025600]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "history_length = 12 # N in article\n",
    "ra_length = 4 # K in article\n",
    "discount_factor = 0.99 # Gamma in Bellman equation\n",
    "actor_lr = 0.0001\n",
    "critic_lr = 0.001\n",
    "tau = 0.001 # τ in Algorithm 3\n",
    "batch_size = 64\n",
    "nb_episodes = 100\n",
    "nb_rounds = 50\n",
    "filename_summary = 'summary.txt'\n",
    "alpha = 0.5 # α (alpha) in Equation (1)\n",
    "gamma = 0.9 # Γ (Gamma) in Equation (4)\n",
    "buffer_size = 1000000 # Size of replay memory D in article\n",
    "fixed_length = True # Fixed memory length\n",
    "\n",
    "\n",
    "dg = DataGenerator(datapath)\n",
    "dg.gen_train_test(0.8, seed=42)\n",
    "\n",
    "print(len(dg.train))\n",
    "print(len(dg.test))\n",
    "print('train: ', dg.train[:10])\n",
    "print('test:', dg.test[:10])\n",
    "\n",
    "#dg.write_csv('books_train.csv', dg.train, nb_states=[history_length], nb_actions=[ra_length])\n",
    "#dg.write_csv('books_test.csv', dg_test, nb_states=[history_length], nb_actions=[ra_length])\n",
    "\n",
    "#data = read_file('books_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
